{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as im\n",
    "import pickle\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import rotate\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score,f1_score,classification_report\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [ 'Scheherazade New' , 'Marhey' , 'Lemonada' , 'IBM Plex Sans Arabic']\n",
    "image_size = 600\n",
    "def show_images(images,titles=None):\n",
    "    #This function is used to show image(s) with titles by sending an array of images and an array of associated titles.\n",
    "    # images[0] will be drawn with the title titles[0] if exists\n",
    "    # You aren't required to understand this function, use it as-is.\n",
    "    n_ims = len(images)\n",
    "    if titles is None: titles = ['(%d)' % i for i in range(1,n_ims + 1)]\n",
    "    fig = plt.figure()\n",
    "    n = 1\n",
    "    for image,title in zip(images,titles):\n",
    "        a = fig.add_subplot(1,n_ims,n)\n",
    "        if image.ndim == 2: \n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "        plt.axis('off')\n",
    "        n += 1\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images from fonts-dataset folder\n",
    "def load_images():\n",
    "    # Load the images from the fonts-dataset folder\n",
    "    images_train = []\n",
    "    labels_train = []\n",
    "    filenames = []\n",
    "    empty_images_filenames = [\"360.jpeg\",\"627.jpeg\",\"853.jpeg\"] \n",
    "    # Use tqdm to show a progress bar\n",
    "    for i in tqdm(labels):\n",
    "        for filename in os.listdir(f'fonts-dataset/{i}'):\n",
    "            img = cv2.imread(f'fonts-dataset/{i}/{filename}', cv2.IMREAD_GRAYSCALE)\n",
    "            # img = cv2.resize(img, (image_size, image_size))\n",
    "            if i == \"Lemonada\" and filename in empty_images_filenames:\n",
    "                print(filename)\n",
    "                print(\"empty image\")\n",
    "                continue\n",
    "            images_train.append(img)\n",
    "            labels_train.append(i)\n",
    "            filenames.append(filename)\n",
    "    return images_train, labels_train,filenames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:14<00:14,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360.jpeg\n",
      "empty image\n",
      "627.jpeg\n",
      "empty image\n",
      "853.jpeg\n",
      "empty image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:28<00:00,  7.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the images\n",
    "X_train, y_train_org, filenames = load_images()\n",
    "# Change the y_train to numbers\n",
    "y_train_org = [labels.index(i) for i in y_train_org]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3997, 3997, 3997)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train_org), len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_score(arr, angle):\n",
    "    \"\"\"\n",
    "    Find the score of the skew angle to be used in deskewing the image\n",
    "    \n",
    "    Args:\n",
    "    arr: the image array\n",
    "    angle: the angle to rotate the image by\n",
    "    \n",
    "    Returns:\n",
    "    hist: the histogram of the image\n",
    "    score: the score of the skew angle\n",
    "    \"\"\"\n",
    "    \n",
    "    # mode{‘reflect’, ‘grid-mirror’, ‘constant’, ‘grid-constant’, ‘nearest’, ‘mirror’, ‘grid-wrap’, ‘wrap’}\n",
    "    data = rotate(arr, angle, reshape=False, order=0, mode='constant', cval=0, prefilter=False)\n",
    "    hist = np.sum(data, axis=1)\n",
    "    score = np.sum((hist[1:] - hist[:-1]) ** 2)\n",
    "    return hist, score\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    \"\"\"\n",
    "    Rotates an image by a given angle and fills the remaining pixels with white color.\n",
    "\n",
    "    Args:\n",
    "        image: A NumPy array representing the input image.\n",
    "        angle: The rotation angle in degrees.\n",
    "\n",
    "    Returns:\n",
    "        A new NumPy array representing the rotated image.\n",
    "    \"\"\"\n",
    "    # Get image height and width\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Compute the rotation matrix\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((width / 2, height / 2), angle, 1)\n",
    "\n",
    "    # Perform the rotation and fill the remaining pixels with white color\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (width, height), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(1, 1, 1))\n",
    "\n",
    "    return rotated_image\n",
    "\n",
    "def deskew(binary_img):\n",
    "    \"\"\"\n",
    "    Deskew the image\n",
    "    \n",
    "    Args:\n",
    "    binary_img: the binary image\n",
    "    \n",
    "    Returns:\n",
    "    pix: the deskewed image\n",
    "    \"\"\"\n",
    "    bin_img = (binary_img // 255.0)\n",
    "    # angles to check for skew angle = 45 degrees and 90 degrees and 180\n",
    "    angles = np.array ([0 , 45 , 90 , 135 , 180 , 225 , 270 , 315])\n",
    "    scores = []\n",
    "    for angle in angles:\n",
    "        hist, score = find_score(bin_img, angle)\n",
    "        scores.append(score)\n",
    "\n",
    "    best_score = max(scores)\n",
    "    best_angle = angles[scores.index(best_score)]\n",
    "    # print('Best angle: {}'.format(best_angle))\n",
    "\n",
    "    # correct skew\n",
    "    # data = rotate(bin_img, best_angle, reshape=False, order=0)\n",
    "    data = rotate_image(bin_img, best_angle)\n",
    "    img = im.fromarray((255 * data).astype(\"uint8\"))\n",
    "\n",
    "    pix = np.array(img)\n",
    "    return pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    \"\"\"\n",
    "    Preprocess the image\n",
    "    \n",
    "    Args:\n",
    "    img: the image\n",
    "    \n",
    "    Returns:\n",
    "    img: the preprocessed image\n",
    "    \"\"\"\n",
    "    sharpen_kernel = np.array([[0,-1, 0], [-1,5,-1], [0,-1,0]])\n",
    "    img = cv2.medianBlur(img, 3) # To remove Salt and Pepper noise\n",
    "    img = cv2.filter2D(img, -1, sharpen_kernel)  # Sharpen the image\n",
    "    img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1] # Convert the image to binary\n",
    "    deskewed_img = deskew(img) # Deskew the image\n",
    "    final_img = cv2.bitwise_not(deskewed_img) if np.mean(deskewed_img) > 127 else deskewed_img # Invert the image if the mean is less than 127 \n",
    "    return final_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess the images\n",
    "# X_train_preprocess = [preprocess(i) for i in tqdm(X_train)]\n",
    "    \n",
    "# # Dump the preprocessed images to a file\n",
    "# with open('preprocessed_images.pkl', 'wb') as f:\n",
    "#     pickle.dump(X_train_preprocess, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed images\n",
    "with open('preprocessed_images.pkl', 'rb') as f:\n",
    "    X_train_preprocess = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "def save_image(img, folder, title):\n",
    "    cv.imwrite(f'./{folder}/{title}.png', img)\n",
    "\n",
    "def projection(gray_img, axis:str='horizontal'):\n",
    "    \"\"\" Compute the horizontal or the vertical projection of a gray image \"\"\"\n",
    "\n",
    "    if axis == 'horizontal':\n",
    "        projection_bins = np.sum(gray_img, 1).astype('int32')\n",
    "    elif axis == 'vertical':\n",
    "        projection_bins = np.sum(gray_img, 0).astype('int32')\n",
    "\n",
    "    return projection_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(image):\n",
    "\n",
    "#     # Maybe we end up using only gray level image.\n",
    "#     # gray_img = cv.bitwise_not(image) # Invert the image\n",
    "#     binary_img = cv.threshold(image, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)[1]\n",
    "#     deskewed_img = deskew(binary_img)\n",
    "#     return deskewed_img\n",
    "\n",
    "\n",
    "def projection_segmentation(clean_img, axis, cut=15, min_width=20, min_height=30):\n",
    "    \"\"\"Segment the image based on the projection profile\n",
    "\n",
    "    Args:\n",
    "        clean_img : Preprocessed image\n",
    "        axis (str): 'horizontal' or 'vertical'\n",
    "        cut (int, optional): Gap between the segments. Defaults to 3.\n",
    "        min_width (int, optional): Width of the segment. Defaults to 5.\n",
    "        min_height (int, optional): Height of the segment. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    start = -1\n",
    "    cnt = 0\n",
    "\n",
    "    projection_bins = projection(clean_img, axis)\n",
    "    for idx, projection_bin in enumerate(projection_bins):\n",
    "\n",
    "        if projection_bin != 0:\n",
    "            cnt = 0\n",
    "        if projection_bin != 0 and start == -1:\n",
    "            start = idx\n",
    "        if projection_bin == 0 and start != -1:\n",
    "            cnt += 1\n",
    "            if cnt >= cut:\n",
    "                if axis == 'horizontal':\n",
    "                    # Line segmentation\n",
    "                    segment = clean_img[max(start-1, 0):idx, :]\n",
    "                    # if segment.shape[0] >= min_height:                    \n",
    "                    segments.append(segment)\n",
    "                elif axis == 'vertical':\n",
    "                    # Word segmentation\n",
    "                    segment = clean_img[:, max(start-1, 0):idx]\n",
    "                    # if segment.shape[1] >= min_width:\n",
    "                    segments.append(segment)\n",
    "                cnt = 0\n",
    "                start = -1\n",
    "    \n",
    "    return segments\n",
    "\n",
    "\n",
    "# Line Segmentation\n",
    "#----------------------------------------------------------------------------------------\n",
    "def line_horizontal_projection(image, cut=3):\n",
    "\n",
    "    # Segmentation    \n",
    "    lines = projection_segmentation(image, axis='horizontal', cut=cut)\n",
    "    return lines\n",
    "\n",
    "\n",
    "# Word Segmentation\n",
    "#----------------------------------------------------------------------------------------\n",
    "def word_vertical_projection(line_image, cut=3):\n",
    "    \n",
    "    line_words = projection_segmentation(line_image, axis='vertical', cut=cut)\n",
    "    line_words.reverse()\n",
    "    return line_words\n",
    "\n",
    "\n",
    "def extract_words(img, visual=0):\n",
    "\n",
    "    lines = line_horizontal_projection(img)\n",
    "    words = []\n",
    "    \n",
    "    for idx, line in enumerate(lines):\n",
    "        \n",
    "        if visual:\n",
    "            # Check for the size of the line to be greater than 30\n",
    "            # if line.shape[0] > 30:\n",
    "            save_image(line, 'lines', f'line{idx}')\n",
    "\n",
    "        line_words = word_vertical_projection(line)\n",
    "        for w in line_words:\n",
    "            # if len(words) == 585:\n",
    "            #     print(idx)\n",
    "            words.append((w, line))\n",
    "        # words.extend(line_words)\n",
    "\n",
    "    # breakpoint()\n",
    "    if visual:\n",
    "        for idx, word in enumerate(words):\n",
    "            # check for the size of the word to be greater than 30\n",
    "            # print (word[0].shape)\n",
    "            # if word[0].shape[0] < 100 and word[0].shape[1] > 20 :\n",
    "            save_image(word[0], 'words', f'word{idx}')\n",
    "    return words\n",
    "\n",
    "# # Try to extract the words from the preprocessed images\n",
    "# x = 31\n",
    "# p = preprocess(X_train[x])\n",
    "\n",
    "# # show_images([X_train[999],p])\n",
    "\n",
    "# # Extract the words from the preprocessed images\n",
    "# words = extract_words(p, visual=1)\n",
    "# show_images([X_train[x], p])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:01<00:00, 3910.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Resize the images to 600x600\n",
    "X_train_resized = []\n",
    "for i in tqdm(X_train_preprocess):\n",
    "    img = cv2.resize(i, (image_size, image_size))\n",
    "    X_train_resized.append(img)\n",
    "X_train_resized = np.array(X_train_resized)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove from X_train_resized with index = [2291, 2587, 2838]\n",
    "X_train_resized = np.delete(X_train_resized, [2291, 2587, 2838], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3997/3997 [02:32<00:00, 26.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3997, 57600)\n"
     ]
    }
   ],
   "source": [
    "# Apply hog for the images\n",
    "from skimage.feature import hog\n",
    "\n",
    "X_train_hog = []\n",
    "for i in tqdm(X_train_resized):\n",
    "    X_train_hog.append(hog(i, orientations= 16, pixels_per_cell=(32, 32), cells_per_block=(4, 4), block_norm='L2-Hys'))\n",
    "    \n",
    "\n",
    "X_train_hog = np.array(X_train_hog)\n",
    "print(X_train_hog.shape)\n",
    "# # Try hog on one image\n",
    "# hog_image = hog(X_train_resized[0], orientations= 16, pixels_per_cell=(32, 32), cells_per_block=(4,4), block_norm='L2-Hys')\n",
    "# print(hog_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3997/3997 [03:37<00:00, 18.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply SIFT for the images\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "X_train_sift = []\n",
    "c=0\n",
    "for i in tqdm(X_train_resized):\n",
    "    kp, des = sift.detectAndCompute(i, None)\n",
    "    if des is None:\n",
    "        # Add a row of zeros to the SIFT descriptors\n",
    "        des = np.zeros((1, 128))\n",
    "    des = des.flatten()\n",
    "    X_train_sift.append(des)\n",
    "    c+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11506.0\n",
      "1265.8418814110582\n",
      "23.0\n"
     ]
    }
   ],
   "source": [
    "# Get the maximum keypoint length of the SIFT descriptors\n",
    "max_kp = max(len(kp)/128 for kp in X_train_sift)\n",
    "\n",
    "# Get the average keypoint length of the SIFT descriptors\n",
    "avg_kp = np.mean([len(kp)/128 for kp in X_train_sift])\n",
    "\n",
    "# Get the minimum keypoint length of the SIFT descriptors\n",
    "min_kp = min(len(kp)/128 for kp in X_train_sift)\n",
    "\n",
    "# Print the maximum keypoint length\n",
    "print(max_kp)\n",
    "\n",
    "# Print the average keypoint length\n",
    "print(avg_kp)\n",
    "\n",
    "# Print the minimum keypoint length\n",
    "print(min_kp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1783, 3780]\n"
     ]
    }
   ],
   "source": [
    "# Get the index of the image with keypoint <= 10\n",
    "idx = [i for i in range(len(X_train_sift)) if len(X_train_sift[i])/128 <= 50]\n",
    "# Print idx \n",
    "print(idx)\n",
    "\n",
    "# Found images which has no text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3997, 38400)\n"
     ]
    }
   ],
   "source": [
    "# Pad the SIFT descriptors to the maximum length\n",
    "fixed_len = 128 * 300  # Adjust this as needed\n",
    "\n",
    "# Create a generator that yields each padded descriptor on-the-fly\n",
    "padded_descriptors = (np.pad(des, (0, max(0, fixed_len - des.shape[0])))[:fixed_len] for des in X_train_sift)\n",
    "\n",
    "# Convert the generator to a numpy array\n",
    "X_train_sift_np = np.array(list(padded_descriptors))\n",
    "\n",
    "# Print the shape of the numpy array\n",
    "print(X_train_sift_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3997, 96000)\n"
     ]
    }
   ],
   "source": [
    "# # Concatenate the hog and sift features\n",
    "X_train_features = np.concatenate((X_train_hog, X_train_sift_np), axis=1)\n",
    "print(X_train_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_features, y_train_org, test_size=0.2, random_state=42, stratify=y_train_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipeline.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a pipeline for the model which consist of StandardScaler , PCA and the model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.99)),\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipe.fit(X_train)\n",
    "\n",
    "# Dump the pipeline to a file\n",
    "joblib.dump(pipe, 'pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pipeline\n",
    "pipe = joblib.load('pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3197, 2984)\n"
     ]
    }
   ],
   "source": [
    "# Transform the data\n",
    "X_train_transformed = pipe.transform(X_train)\n",
    "X_test_transformed = pipe.transform(X_test)\n",
    "\n",
    "# Print the shape of the transformed data\n",
    "print(X_train_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  100.0\n",
      "Test accuracy:  92.75\n",
      "Train classification report:                        precision    recall  f1-score   support\n",
      "\n",
      "    Scheherazade New       1.00      1.00      1.00       800\n",
      "              Marhey       1.00      1.00      1.00       800\n",
      "            Lemonada       1.00      1.00      1.00       797\n",
      "IBM Plex Sans Arabic       1.00      1.00      1.00       800\n",
      "\n",
      "            accuracy                           1.00      3197\n",
      "           macro avg       1.00      1.00      1.00      3197\n",
      "        weighted avg       1.00      1.00      1.00      3197\n",
      "\n",
      "Test classification report:                        precision    recall  f1-score   support\n",
      "\n",
      "    Scheherazade New       0.92      0.91      0.91       200\n",
      "              Marhey       0.92      0.93      0.93       200\n",
      "            Lemonada       0.98      0.98      0.98       200\n",
      "IBM Plex Sans Arabic       0.89      0.89      0.89       200\n",
      "\n",
      "            accuracy                           0.93       800\n",
      "           macro avg       0.93      0.93      0.93       800\n",
      "        weighted avg       0.93      0.93      0.93       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build a shallow neural network model of 1 hidden layer with 128 neurons and relu activation function\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a model\n",
    "model = MLPClassifier(hidden_layer_sizes=(256,), max_iter=1000, activation='relu', solver='adam', \n",
    "                      random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict the training data\n",
    "y_train_pred = model.predict(X_train_transformed)\n",
    "\n",
    "# Predict the testing data\n",
    "y_test_pred = model.predict(X_test_transformed)\n",
    "\n",
    "# Print the accuracy of the model\n",
    "print('Train accuracy: ', accuracy_score(y_train, y_train_pred)*100)\n",
    "\n",
    "print('Test accuracy: ', accuracy_score(y_test, y_test_pred)*100)\n",
    "\n",
    "# Print the classification report\n",
    "print('Train classification report: ', classification_report(y_train, y_train_pred, target_names=labels))\n",
    "\n",
    "print('Test classification report: ', classification_report(y_test, y_test_pred, target_names=labels))\n",
    "\n",
    "# Accuracy: 92.75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lin_model = LogisticRegression()\n",
    "\n",
    "# # Define the hyperparameters\n",
    "# param_grid = {\n",
    "#     'C': np.logspace(-4, 4, 30),\n",
    "#     'penalty': ['l2'],\n",
    "#     'solver': ['liblinear', 'saga', 'lbfgs'],\n",
    "#     'warm_start': [True, False]\n",
    "# }\n",
    "\n",
    "# # Initialize the RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(lin_model, param_distributions=param_grid, n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# # Fit the model\n",
    "# random_search.fit(X_train_transformed, y_train)\n",
    "# # cv_results_df = pd.DataFrame(clf_searched.cv_results_)[relevant_columns].round(decimals=3).sort_values(by='rank_test_score')\n",
    "# # cv_results_df.head(10)\n",
    "# relevant_columns = ['param_C', 'param_penalty', 'param_solver', 'param_warm_start', 'mean_test_score', 'std_test_score', 'rank_test_score']\n",
    "# cv_results_df = pd.DataFrame(random_search.cv_results_)[relevant_columns].round(decimals=3).sort_values(by='rank_test_score')\n",
    "# cv_results_df.head(10)\n",
    "\n",
    "# # Print the best parameters\n",
    "# print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 100.0\n",
      "Testing Accuracy: 95.375\n",
      "Training F1 Score: 1.0\n",
      "Testing F1 Score: 0.9538616748349624\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "    Scheherazade New       1.00      1.00      1.00       800\n",
      "              Marhey       1.00      1.00      1.00       800\n",
      "            Lemonada       1.00      1.00      1.00       797\n",
      "IBM Plex Sans Arabic       1.00      1.00      1.00       800\n",
      "\n",
      "            accuracy                           1.00      3197\n",
      "           macro avg       1.00      1.00      1.00      3197\n",
      "        weighted avg       1.00      1.00      1.00      3197\n",
      "\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "    Scheherazade New       0.96      0.94      0.95       200\n",
      "              Marhey       0.91      0.94      0.93       200\n",
      "            Lemonada       0.99      0.99      0.99       200\n",
      "IBM Plex Sans Arabic       0.95      0.94      0.94       200\n",
      "\n",
      "            accuracy                           0.95       800\n",
      "           macro avg       0.95      0.95      0.95       800\n",
      "        weighted avg       0.95      0.95      0.95       800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['logistic_model.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model and fit the data\n",
    "\n",
    "# Initialize the model\n",
    "# {'warm_start': True, 'solver': 'saga', 'penalty': 'l2', 'C': 0.001}\n",
    "\n",
    "# {'warm_start': True, 'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.0006723357536499335}\n",
    "model = LogisticRegression(warm_start=True, solver='saga', penalty='l2', C=0.8, random_state=42)\n",
    "# model = LogisticRegression()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict the test data\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "\n",
    "# Predict the train data\n",
    "y_pred_train = model.predict(X_train_transformed)\n",
    "\n",
    "# Print the training accuracy\n",
    "print(f\"Training Accuracy: {accuracy_score(y_train, y_pred_train)*100}\")\n",
    "\n",
    "# Print the testing accuracy\n",
    "print(f\"Testing Accuracy: {accuracy_score(y_test, y_pred)*100}\")\n",
    "\n",
    "# Print the f1 score of the training data\n",
    "print(f\"Training F1 Score: {f1_score(y_train, y_pred_train, average='weighted')}\")\n",
    "\n",
    "# Print the f1 score of the testing data\n",
    "print (f\"Testing F1 Score: {f1_score(y_test, y_pred, average='weighted')}\")\n",
    "\n",
    "# Print the classification report of the training data\n",
    "print(classification_report(y_train, y_pred_train, target_names=labels))\n",
    "\n",
    "# Print the classification report of the testing data\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, 'logistic_model.pkl')\n",
    "\n",
    "#ACCUARCY: 95.375%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_kernel</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>7.196857</td>\n",
       "      <td>rbf</td>\n",
       "      <td>scale</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>19.306977</td>\n",
       "      <td>rbf</td>\n",
       "      <td>scale</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>372.759372</td>\n",
       "      <td>rbf</td>\n",
       "      <td>scale</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>51.794747</td>\n",
       "      <td>rbf</td>\n",
       "      <td>scale</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>19.306977</td>\n",
       "      <td>rbf</td>\n",
       "      <td>scale</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.196857</td>\n",
       "      <td>rbf</td>\n",
       "      <td>scale</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>rbf</td>\n",
       "      <td>scale</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>372.759372</td>\n",
       "      <td>rbf</td>\n",
       "      <td>scale</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>138.949549</td>\n",
       "      <td>rbf</td>\n",
       "      <td>scale</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>138.949549</td>\n",
       "      <td>rbf</td>\n",
       "      <td>scale</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       param_C param_kernel param_gamma  mean_test_score  std_test_score  \\\n",
       "73    7.196857          rbf       scale            0.933            0.01   \n",
       "45   19.306977          rbf       scale            0.933            0.01   \n",
       "95  372.759372          rbf       scale            0.933            0.01   \n",
       "84   51.794747          rbf       scale            0.933            0.01   \n",
       "80   19.306977          rbf       scale            0.933            0.01   \n",
       "6     7.196857          rbf       scale            0.933            0.01   \n",
       "71      1000.0          rbf       scale            0.933            0.01   \n",
       "16  372.759372          rbf       scale            0.933            0.01   \n",
       "88  138.949549          rbf       scale            0.933            0.01   \n",
       "29  138.949549          rbf       scale            0.933            0.01   \n",
       "\n",
       "    rank_test_score  \n",
       "73                1  \n",
       "45                1  \n",
       "95                1  \n",
       "84                1  \n",
       "80                1  \n",
       "6                 1  \n",
       "71                1  \n",
       "16                1  \n",
       "88                1  \n",
       "29                1  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# svm = SVC()\n",
    "\n",
    "# param_dist = {\n",
    "#     'C': np.logspace(-3, 3, 15), \n",
    "#     'kernel': ['poly', 'rbf'], \n",
    "#     'degree': [2, 3],\n",
    "#     'gamma': ['scale', 'auto']\n",
    "# }\n",
    "\n",
    "# clf_searched = RandomizedSearchCV(svm, param_dist, n_iter=100, cv=5, random_state=42, n_jobs=-1, verbose=1)\n",
    "\n",
    "# clf_searched.fit(X_train_transformed, y_train)\n",
    "# relevant_columns = ['param_C', 'param_kernel', 'param_gamma', 'mean_test_score', 'std_test_score', 'rank_test_score']\n",
    "# cv_results_df = pd.DataFrame(clf_searched.cv_results_)[relevant_columns].round(decimals=3).sort_values(by='rank_test_score')\n",
    "# cv_results_df.head(10)\n",
    "\n",
    "# Best parameters of the SVM\n",
    "# {C : 7.196857 , kernel : 'rbf', gamma : 'scale'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM model on the training set: 100.0\n",
      "Accuracy of SVM model: 90.125\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "    Scheherazade New       1.00      1.00      1.00       800\n",
      "              Marhey       1.00      1.00      1.00       800\n",
      "            Lemonada       1.00      1.00      1.00       797\n",
      "IBM Plex Sans Arabic       1.00      1.00      1.00       800\n",
      "\n",
      "            accuracy                           1.00      3197\n",
      "           macro avg       1.00      1.00      1.00      3197\n",
      "        weighted avg       1.00      1.00      1.00      3197\n",
      "\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "    Scheherazade New       0.99      0.66      0.79       200\n",
      "              Marhey       0.87      0.96      0.91       200\n",
      "            Lemonada       1.00      1.00      1.00       200\n",
      "IBM Plex Sans Arabic       0.80      0.99      0.88       200\n",
      "\n",
      "            accuracy                           0.90       800\n",
      "           macro avg       0.92      0.90      0.90       800\n",
      "        weighted avg       0.92      0.90      0.90       800\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the SVM model\n",
    "svm = SVC(C=7.196857 , kernel='rbf', gamma='scale', random_state=42)\n",
    "\n",
    "# Fit the SVM on the training data\n",
    "svm.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = svm.predict(X_test_transformed)\n",
    "# Predict the labels of the training set\n",
    "y_pred_train = svm.predict(X_train_transformed)\n",
    "\n",
    "# Print the accuracy of the SVM model on the training set\n",
    "print(f\"Accuracy of SVM model on the training set: {accuracy_score(y_train, y_pred_train)*100}\")\n",
    "\n",
    "# Print the accuracy of the SVM model on the test set\n",
    "print(f\"Accuracy of SVM model: {accuracy_score(y_test, y_pred)*100}\")\n",
    "\n",
    "# Print the classification report of the SVM model\n",
    "print(classification_report(y_train, y_pred_train, target_names=labels))\n",
    "\n",
    "# Print the classification report of the SVM model\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "# Save the SVM model as h5 file\n",
    "joblib.dump(svm, 'svm_model.pkl')\n",
    "\n",
    "#ACCURACY: 91.25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random Forest Classifier\n",
    "\n",
    "# random_forest_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # Define the hyperparameters distribution (use same ranges as before)\n",
    "# param_dist = {\n",
    "#     'n_estimators': np.arange(200, 500), \n",
    "#     'min_samples_split':  range(2,51),                    # 2 to 50\n",
    "#     'min_samples_leaf': range(1,21),                      # 1 to 20\n",
    "#     'max_depth': range(5, 51, 5),                              # 5 to 50 with step of 5\n",
    "#     'min_impurity_decrease': np.linspace(0.0, 0.2, 20),                   # Decide a reasonable range here (with 20 values)\n",
    "# }\n",
    "\n",
    "# # Initialize RandomizedSearchCV\n",
    "# clf_searched = RandomizedSearchCV(random_forest_clf, param_dist, n_iter=100, cv=5, random_state=42, n_jobs=-1, verbose=1)\n",
    "\n",
    "# clf_searched.fit(X_train_transformed, y_train)\n",
    "# relevant_columns = ['param_n_estimators', 'param_min_samples_split', 'param_min_samples_leaf', 'param_max_depth', 'param_min_impurity_decrease', \n",
    "#                     'mean_test_score', 'std_test_score', 'rank_test_score']\n",
    "# cv_results_df = pd.DataFrame(clf_searched.cv_results_)[relevant_columns].round(decimals=3).sort_values(by='rank_test_score')\n",
    "# cv_results_df.head(10)\n",
    "\n",
    "# # Best parameters of the Random Forest Classifier\n",
    "# # {'n_estimators': 371, 'min_samples_split': 48, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.0, 'max_depth': 30}\n",
    "# # Best score: 0.776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest Classifier model on the training set: 99.96875\n",
      "Accuracy of Random Forest Classifier model: 83.25\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "    Scheherazade New       1.00      1.00      1.00       800\n",
      "              Marhey       1.00      1.00      1.00       800\n",
      "            Lemonada       1.00      1.00      1.00       800\n",
      "IBM Plex Sans Arabic       1.00      1.00      1.00       800\n",
      "\n",
      "            accuracy                           1.00      3200\n",
      "           macro avg       1.00      1.00      1.00      3200\n",
      "        weighted avg       1.00      1.00      1.00      3200\n",
      "\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "    Scheherazade New       0.92      0.71      0.81       200\n",
      "              Marhey       0.67      0.95      0.79       200\n",
      "            Lemonada       0.96      1.00      0.98       200\n",
      "IBM Plex Sans Arabic       0.86      0.67      0.75       200\n",
      "\n",
      "            accuracy                           0.83       800\n",
      "           macro avg       0.85      0.83      0.83       800\n",
      "        weighted avg       0.85      0.83      0.83       800\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['random_forest_clf.pkl']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_clf = RandomForestClassifier(n_estimators=371, min_samples_split=48, min_samples_leaf=4, min_impurity_decrease=0., max_depth=30, random_state=42)\n",
    "\n",
    "# Fit the Random Forest Classifier\n",
    "random_forest_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = random_forest_clf.predict(X_test_transformed)\n",
    "\n",
    "# Predict the labels of the training set\n",
    "y_pred_train = random_forest_clf.predict(X_train_transformed)\n",
    "\n",
    "# Print the accuracy of the Random Forest Classifier model on the training set\n",
    "print(f\"Accuracy of Random Forest Classifier model on the training set: {accuracy_score(y_train, y_pred_train)*100}\")\n",
    "\n",
    "# Print the accuracy of the Random Forest Classifier model on the test set\n",
    "print(f\"Accuracy of Random Forest Classifier model: {accuracy_score(y_test, y_pred)*100}\")\n",
    "\n",
    "# Print the classification report of the Random Forest Classifier model\n",
    "print(classification_report(y_train, y_pred_train, target_names=labels))\n",
    "\n",
    "# Print the classification report of the Random Forest Classifier model\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "# Save the Random Forest Classifier model as h5 file\n",
    "joblib.dump(random_forest_clf, 'random_forest_clf.pkl')\n",
    "\n",
    "#ACCURACY: 83.25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3197, 2984]) torch.Size([3197]) torch.Size([800, 2984]) torch.Size([800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<01:38,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3966382145881653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:01<01:28,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.04180854186415672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:02<01:26,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.005950699094682932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:03<01:27,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.0030725402757525444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:04<01:27,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.001876615104265511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:05<01:24,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.0016626014839857817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:06<01:22,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.0014224061742424965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:07<01:20,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.0007364930352196097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:08<01:18,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.0005590920336544514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:08<01:17,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.0003658164059743285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:09<01:16,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.0004937460180372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:10<01:17,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 0.0003923581098206341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:11<01:15,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 0.0003619663475546986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:12<01:14,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 0.00028472140547819436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:13<01:12,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 0.00018523944891057909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:14<01:11,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 0.00020199721620883793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:14<01:10,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 0.0001640623522689566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [00:15<01:09,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 0.00012937464634887874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [00:16<01:09,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 0.00013986659178044647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:17<01:11,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 0.00011749611439881846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:18<01:08,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 0.0001647258031880483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:19<01:06,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss: 9.03828622540459e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [00:20<01:04,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Loss: 7.5092728366144e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [00:20<01:02,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Loss: 8.201754098990932e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [00:21<01:01,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Loss: 6.974231655476615e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [00:22<01:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Loss: 6.125826621428132e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [00:23<00:59,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Loss: 6.422504520742223e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [00:24<01:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Loss: 4.5232231059344485e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [00:24<00:58,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Loss: 4.566370262182318e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:25<00:57,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: 3.7397399864858016e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:26<00:56,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Loss: 4.101844024262391e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:27<00:55,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Loss: 3.9551501686219126e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:28<00:54,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Loss: 3.0541552405338734e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [00:28<00:53,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Loss: 3.0488105039694346e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [00:29<00:52,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Loss: 3.23294589179568e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [00:30<00:53,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Loss: 2.5543027732055634e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [00:31<00:51,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Loss: 2.3323520508711226e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [00:32<00:50,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Loss: 2.132991903636139e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [00:33<00:49,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Loss: 2.3594502636115067e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [00:33<00:48,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Loss: 2.1469579223776236e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [00:34<00:47,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Loss: 1.5131189684325363e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [00:35<00:46,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Loss: 1.542301833978854e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [00:36<00:45,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, Loss: 1.9858189261867665e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [00:37<00:46,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, Loss: 1.495855667599244e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [00:37<00:44,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Loss: 1.1415191693231463e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [00:38<00:44,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Loss: 1.726037953631021e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [00:39<00:43,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Loss: 9.89018280961318e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [00:40<00:42,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Loss: 1.219199566548923e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [00:41<00:41,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Loss: 1.3515703358280007e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [00:42<00:40,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 7.333389476116281e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [00:42<00:39,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, Loss: 9.49966397456592e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [00:43<00:39,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, Loss: 1.2163343853899278e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [00:44<00:38,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53, Loss: 8.01574788056314e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [00:45<00:37,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54, Loss: 8.023955160751939e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [00:46<00:36,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55, Loss: 7.904753147158772e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [00:46<00:35,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56, Loss: 8.311706551467068e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [00:47<00:35,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57, Loss: 5.4466099754790775e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [00:48<00:34,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58, Loss: 5.541152859223075e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [00:49<00:33,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59, Loss: 8.14728900877526e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [00:50<00:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, Loss: 5.454804067994701e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [00:51<00:32,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61, Loss: 4.842345333599951e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [00:51<00:31,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62, Loss: 3.6297060432843864e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [00:52<00:30,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63, Loss: 4.028440343972761e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [00:53<00:29,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64, Loss: 4.603925845003687e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [00:54<00:28,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, Loss: 3.707810037667514e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [00:55<00:27,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66, Loss: 5.138291271578055e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [00:56<00:27,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67, Loss: 4.464163794182241e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [00:56<00:26,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68, Loss: 3.7859115309402114e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [00:57<00:25,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69, Loss: 4.20108653997886e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [00:58<00:24,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70, Loss: 3.0295527722046245e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [00:59<00:23,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71, Loss: 3.0172200240485836e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [01:00<00:23,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72, Loss: 2.5609399472159566e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [01:00<00:22,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73, Loss: 2.96377993436181e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [01:01<00:21,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74, Loss: 1.882681999632041e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [01:02<00:20,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Loss: 2.301967924722703e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [01:03<00:20,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76, Loss: 2.0717714050988434e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [01:04<00:19,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77, Loss: 2.116988298439537e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [01:05<00:18,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78, Loss: 2.1991993435221957e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [01:05<00:17,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79, Loss: 1.8991242995980429e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [01:06<00:16,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, Loss: 1.648374336582492e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [01:07<00:15,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81, Loss: 1.311300820816541e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [01:08<00:14,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, Loss: 1.4880590697430307e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [01:09<00:14,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83, Loss: 1.4346209127324983e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [01:10<00:13,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84, Loss: 1.483948608438368e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [01:11<00:13,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85, Loss: 1.122210505855037e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [01:11<00:12,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, Loss: 1.1263209671596996e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [01:12<00:11,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87, Loss: 9.577843229635619e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [01:13<00:10,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88, Loss: 1.1468740694908774e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [01:14<00:09,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89, Loss: 1.0030013299910934e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [01:15<00:08,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, Loss: 9.043458248925162e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [01:16<00:07,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91, Loss: 6.24820700068085e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [01:16<00:06,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92, Loss: 7.44030103305704e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [01:17<00:05,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, Loss: 9.701155931907124e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [01:18<00:04,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94, Loss: 6.65927530008048e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [01:19<00:04,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95, Loss: 5.960462772236497e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [01:20<00:03,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96, Loss: 7.070339620440791e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [01:20<00:02,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97, Loss: 6.207101819200034e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [01:21<00:01,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98, Loss: 7.933579126984114e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [01:22<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, Loss: 5.015010060560599e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:23<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 5.878245019630413e-07\n",
      "Accuracy:  96.375\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "    Scheherazade New       0.96      0.96      0.96       200\n",
      "              Marhey       0.95      0.94      0.94       200\n",
      "            Lemonada       0.99      0.99      0.99       200\n",
      "IBM Plex Sans Arabic       0.96      0.96      0.96       200\n",
      "\n",
      "            accuracy                           0.96       800\n",
      "           macro avg       0.96      0.96      0.96       800\n",
      "        weighted avg       0.96      0.96      0.96       800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Build a neural networ consist of input layer of size 400, hidden layer of size 256 , hidden layer of size 128 and output layer of size 4 pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert the data to tensors\n",
    "X_train_tensor = torch.tensor(X_train_transformed, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_transformed, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Print the shape of the tensors\n",
    "print(X_train_tensor.shape, y_train_tensor.shape, X_test_tensor.shape, y_test_tensor.shape)\n",
    "\n",
    "# Create a dataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create a dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2984, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "# Create the model\n",
    "model = Net()\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0002)\n",
    "\n",
    "# Train the model\n",
    "for epoch in tqdm(range(100)):\n",
    "    model.train()\n",
    "    for X, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    \n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "\n",
    "# Get the predictions\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for X, y in test_loader:\n",
    "    output = model(X)\n",
    "    y_pred.extend(torch.argmax(output, 1).tolist())\n",
    "    y_true.extend(y.tolist())\n",
    "    \n",
    "# Print the accuracy\n",
    "print('Accuracy: ', accuracy_score(y_true, y_pred)*100)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_true, y_pred, target_names=labels))\n",
    "# Accuracy:  96.375%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stacking Classifier model on the training set: 100.0\n",
      "Accuracy of Stacking Classifier model: 96.0\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "    Scheherazade New       1.00      1.00      1.00       800\n",
      "              Marhey       1.00      1.00      1.00       800\n",
      "            Lemonada       1.00      1.00      1.00       800\n",
      "IBM Plex Sans Arabic       1.00      1.00      1.00       800\n",
      "\n",
      "            accuracy                           1.00      3200\n",
      "           macro avg       1.00      1.00      1.00      3200\n",
      "        weighted avg       1.00      1.00      1.00      3200\n",
      "\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "    Scheherazade New       0.97      0.92      0.95       200\n",
      "              Marhey       0.95      0.95      0.95       200\n",
      "            Lemonada       0.99      1.00      0.99       200\n",
      "IBM Plex Sans Arabic       0.93      0.97      0.95       200\n",
      "\n",
      "            accuracy                           0.96       800\n",
      "           macro avg       0.96      0.96      0.96       800\n",
      "        weighted avg       0.96      0.96      0.96       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stacking the models together to get the best model\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Define the base models\n",
    "estimators = [\n",
    "    ('logistic', LogisticRegression(warm_start=True, solver='saga', penalty='l2', C=0.8, random_state=42)),\n",
    "    ('svm', SVC(C=7.196857 , kernel='rbf', gamma='scale', random_state=42)),\n",
    "    # ('mlp', MLPClassifier(hidden_layer_sizes=(256,), max_iter=1000, activation='relu', solver='adam', \n",
    "    #                 random_state=42))\n",
    "]\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Initialize the stacking classifier\n",
    "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=meta_model, cv=5)\n",
    "\n",
    "# Fit the stacking classifier\n",
    "stacking_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = stacking_clf.predict(X_test_transformed)\n",
    "\n",
    "# Predict the labels of the training set\n",
    "y_pred_train = stacking_clf.predict(X_train_transformed)\n",
    "\n",
    "# Print the accuracy of the Stacking Classifier model on the training set\n",
    "print(f\"Accuracy of Stacking Classifier model on the training set: {accuracy_score(y_train, y_pred_train)*100}\")\n",
    "\n",
    "# Print the accuracy of the Stacking Classifier model on the test set\n",
    "print(f\"Accuracy of Stacking Classifier model: {accuracy_score(y_test, y_pred)*100}\")\n",
    "\n",
    "# Print the classification report of the Stacking Classifier model\n",
    "print(classification_report(y_train, y_pred_train, target_names=labels))\n",
    "\n",
    "# Print the classification report of the Stacking Classifier model\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "# ACCURACY: 96%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
