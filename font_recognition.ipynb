{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“– Loading Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import interpolation as inter\n",
    "from PIL import Image as im\n",
    "import pickle\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import rotate\n",
    "from sklearn.metrics import accuracy_score,f1_score,classification_report\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.signal import convolve2d\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from skimage.feature import hog\n",
    "from copy import deepcopy\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import plotly.express as px\n",
    "from utils import *\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”§ Utilities Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images,titles=None):\n",
    "    \"\"\"\n",
    "    This function is used to show image(s) with titles by sending an array of images and an array of associated titles.\n",
    "    images[0] will be drawn with the title titles[0] if exists.\n",
    "    \"\"\"\n",
    "    n_ims = len(images)\n",
    "    if titles is None: titles = ['(%d)' % i for i in range(1,n_ims + 1)]\n",
    "    fig = plt.figure()\n",
    "    n = 1\n",
    "    for image,title in zip(images,titles):\n",
    "        a = fig.add_subplot(1,n_ims,n)\n",
    "        if image.ndim == 2: \n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "        plt.axis('off')\n",
    "        n += 1\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images():\n",
    "    \"\"\"\n",
    "    This function is used to load the images from the fonts-dataset folder.\n",
    "    \"\"\"\n",
    "    images_train = []\n",
    "    labels_train = []\n",
    "    filenames = []\n",
    "    labels = ['Scheherazade New', 'Marhey', 'Lemonada', 'IBM Plex Sans Arabic']\n",
    "    empty_images_filenames = [\"360.jpeg\",\"627.jpeg\",\"853.jpeg\"] \n",
    "    for i in tqdm(labels):\n",
    "        for filename in os.listdir(f'fonts-dataset/{i}'):\n",
    "            img = cv2.imread(f'fonts-dataset/{i}/{filename}', cv2.IMREAD_GRAYSCALE)\n",
    "            if i == \"Lemonada\" and filename in empty_images_filenames:\n",
    "                print(f\"{filename} is empty image!\")\n",
    "                continue\n",
    "            images_train.append(img)\n",
    "            labels_train.append(i)\n",
    "            filenames.append(filename)\n",
    "    return images_train, labels_train, filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš™ï¸ Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_score(arr, angle):\n",
    "    \"\"\"\n",
    "    Find the score of the skew angle to be used in deskewing the image\n",
    "    \n",
    "    Args:\n",
    "    arr: the image array\n",
    "    angle: the angle to rotate the image by\n",
    "    \n",
    "    Returns:\n",
    "    hist: the histogram of the image\n",
    "    score: the score of the skew angle\n",
    "    \"\"\"\n",
    "    \n",
    "    # mode{â€˜reflectâ€™, â€˜grid-mirrorâ€™, â€˜constantâ€™, â€˜grid-constantâ€™, â€˜nearestâ€™, â€˜mirrorâ€™, â€˜grid-wrapâ€™, â€˜wrapâ€™}\n",
    "    data = rotate(arr, angle, reshape=False, order=0, mode='constant', cval=0, prefilter=False)\n",
    "    hist = np.sum(data, axis=1)\n",
    "    score = np.sum((hist[1:] - hist[:-1]) ** 2)\n",
    "    return hist, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_image(image, angle):\n",
    "    \"\"\"\n",
    "    Rotates an image by a given angle and fills the remaining pixels with white color.\n",
    "\n",
    "    Args:\n",
    "        image: A NumPy array representing the input image.\n",
    "        angle: The rotation angle in degrees.\n",
    "\n",
    "    Returns:\n",
    "        A new NumPy array representing the rotated image.\n",
    "    \"\"\"\n",
    "    # Get image height and width\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Compute the rotation matrix\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((width / 2, height / 2), angle, 1)\n",
    "\n",
    "    # Perform the rotation and fill the remaining pixels with white color\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (width, height), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
    "\n",
    "    return rotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deskew(binary_img):\n",
    "    \"\"\"\n",
    "    Deskew the image\n",
    "    \n",
    "    Args:\n",
    "    binary_img: the binary image\n",
    "    \n",
    "    Returns:\n",
    "    pix: the deskewed image\n",
    "    \"\"\"\n",
    "    bin_img = (binary_img // 255.0)\n",
    "    # angles to check for skew angle = 45 degrees and 90 degrees and 180\n",
    "    angles = np.array ([0 , 45 , 90 , 135 , 180 , 225 , 270 , 315])\n",
    "    scores = []\n",
    "    for angle in angles:\n",
    "        hist, score = find_score(bin_img, angle)\n",
    "        scores.append(score)\n",
    "\n",
    "    best_score = max(scores)\n",
    "    best_angle = angles[scores.index(best_score)]\n",
    "    # print('Best angle: {}'.format(best_angle))\n",
    "\n",
    "    # correct skew\n",
    "    # data = rotate(bin_img, best_angle, reshape=False, order=0)\n",
    "    data = rotate_image(bin_img, best_angle)\n",
    "    img = im.fromarray((255 * data).astype(\"uint8\"))\n",
    "\n",
    "    pix = np.array(img)\n",
    "    return pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    \"\"\"\n",
    "    Preprocess the image\n",
    "    \n",
    "    Args:\n",
    "    img: the image\n",
    "    \n",
    "    Returns:\n",
    "    img: the preprocessed image\n",
    "    \"\"\"\n",
    "    image_size = 600\n",
    "    sharpen_kernel = np.array([[0,-1, 0], [-1,5,-1], [0,-1,0]])\n",
    "    img = cv2.medianBlur(img, 3) # To remove Salt and Pepper noise\n",
    "    img = cv2.filter2D(img, -1, sharpen_kernel)  # Sharpen the image\n",
    "    img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1] # Convert the image to binary\n",
    "    img = cv2.bitwise_not(img) if np.mean(img) > 127 else img # Invert the image if the mean is less than 127 \n",
    "    img = deskew(img) # Deskew the image\n",
    "    final_img = cv2.resize(img, (image_size, image_size)) # Resize the image\n",
    "    # final_img = variance_threshold(final_img)\n",
    "    return final_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  âœ˜ Unsuccessful Preprocessing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation technique (Insipred by Variance Threshold by scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rows(image, threshold=0.008):\n",
    "    \"\"\"\n",
    "    Remove rows with white pixels less than 10% of the image size\n",
    "    \n",
    "    Args:\n",
    "    X_preprocess_sliced: the list of images\n",
    "    max_shape_index: the index of the image to process\n",
    "    \n",
    "    Returns:\n",
    "    image: the image after removing the rows\n",
    "    \"\"\"\n",
    "    white_pixels_per_row = np.sum(image == 255, axis=1)\n",
    "    rows_to_remove = white_pixels_per_row < image.shape[1] * threshold\n",
    "    image = image[~rows_to_remove]\n",
    "    return image\n",
    "\n",
    "def remove_columns(image, threshold = 0.1):\n",
    "    \"\"\"\n",
    "    Remove columns with white pixels less than 10% of the image size\n",
    "    \n",
    "    Args:\n",
    "    X_preprocess_sliced: the list of images\n",
    "    max_shape_index: the index of the image to process\n",
    "    \n",
    "    Returns:\n",
    "    image: the image after removing the columns\n",
    "    \"\"\"\n",
    "    white_pixels_per_column = np.sum(image == 255, axis=0)\n",
    "    # white < 0.1 * height\n",
    "    columns_to_remove = white_pixels_per_column < image.shape[0] * threshold\n",
    "    image = image[:, ~columns_to_remove]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_image(image):\n",
    "    \"\"\"\n",
    "    Pad the image with zeros if the width is less than 515 and the height is less than 270\n",
    "    \n",
    "    Args:\n",
    "    image: the image\n",
    "    \n",
    "    Returns:\n",
    "    image: the padded image\n",
    "    \"\"\"\n",
    "    if image.shape[1] < 515:\n",
    "        pad_width = 515 - image.shape[1]\n",
    "        image = np.pad(image, ((0, 0), (0, pad_width)), 'constant', constant_values=(0, 0))\n",
    "    if image.shape[0] < 270:\n",
    "        pad_height = 270 - image.shape[0]\n",
    "        image = np.pad(image, ((0, pad_height), (0, 0)), 'constant', constant_values=(0, 0))\n",
    "    return image\n",
    "\n",
    "def crop_image(image):\n",
    "    \"\"\"\n",
    "    Crop the image if the width is more than 515 and the height is more than 270\n",
    "    \n",
    "    Args:\n",
    "    image: the image\n",
    "    \n",
    "    Returns:\n",
    "    image: the cropped image\n",
    "    \"\"\"\n",
    "    if image.shape[1] > 515:\n",
    "        crop_width = image.shape[1] - 515\n",
    "        image = image[:, crop_width//2:-(crop_width//2)]\n",
    "    if image.shape[0] > 270:\n",
    "        crop_height = image.shape[0] - 270\n",
    "        image = image[crop_height//2:-(crop_height//2), :]\n",
    "    return cv2.resize(image, (515, 270))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_new(img, columns_threshold=0.001, rows_threshold=0.005):\n",
    "    \"\"\"\n",
    "    Preprocess the image\n",
    "    \n",
    "    Args:\n",
    "    img: the image\n",
    "    \n",
    "    Returns:\n",
    "    img: the preprocessed image\n",
    "    \"\"\"\n",
    "    image_size = 600\n",
    "    sharpen_kernel = np.array([[0,-1, 0], [-1,5,-1], [0,-1,0]])\n",
    "    img = cv2.medianBlur(img, 3) # To remove Salt and Pepper noise\n",
    "    img = cv2.filter2D(img, -1, sharpen_kernel)  # Sharpen the image\n",
    "    img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1] # Convert the image to binary\n",
    "    img = cv2.bitwise_not(img) if np.mean(img) > 127 else img # Invert the image if the mean is less than 127 \n",
    "    img = deskew(img) # Deskew the image\n",
    "    img = cv2.resize(img, (image_size, image_size)) # Resize the image\n",
    "    img = remove_columns(img, columns_threshold)\n",
    "    img = remove_rows(img, rows_threshold)\n",
    "    img = pad(img)\n",
    "    final_img = crop_image(img)\n",
    "    return final_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Thresholding by scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_threshold(image):\n",
    "    selector = VarianceThreshold(10)\n",
    "    returned_var_column = selector.fit_transform(image)\n",
    "    returned_var_column = returned_var_column.T\n",
    "    returned_var_column_row = selector.fit_transform(returned_var_column)\n",
    "    returned_var = returned_var_column_row.T\n",
    "    return returned_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line & Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(img, folder, title):\n",
    "    cv2.imwrite(f'./{folder}/{title}.png', img)\n",
    "    \n",
    "def projection(gray_img, axis:str='horizontal'):\n",
    "    \"\"\" \n",
    "    Compute the horizontal or the vertical projection of a gray image \n",
    "    \"\"\"\n",
    "    if axis == 'horizontal':\n",
    "        projection_bins = np.sum(gray_img, 1).astype('int32')\n",
    "    elif axis == 'vertical':\n",
    "        projection_bins = np.sum(gray_img, 0).astype('int32')\n",
    "\n",
    "    return projection_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection_segmentation(clean_img, axis, cut=15, min_width=20, min_height=30):\n",
    "    \"\"\"Segment the image based on the projection profile\n",
    "\n",
    "    Args:\n",
    "        clean_img : Preprocessed image\n",
    "        axis (str): 'horizontal' or 'vertical'\n",
    "        cut (int, optional): Gap between the segments. Defaults to 3.\n",
    "        min_width (int, optional): Width of the segment. Defaults to 5.\n",
    "        min_height (int, optional): Height of the segment. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    start = -1\n",
    "    cnt = 0\n",
    "\n",
    "    projection_bins = projection(clean_img, axis)\n",
    "    for idx, projection_bin in enumerate(projection_bins):\n",
    "\n",
    "        if projection_bin != 0:\n",
    "            cnt = 0\n",
    "        if projection_bin != 0 and start == -1:\n",
    "            start = idx\n",
    "        if projection_bin == 0 and start != -1:\n",
    "            cnt += 1\n",
    "            if cnt >= cut:\n",
    "                if axis == 'horizontal':\n",
    "                    # Line segmentation\n",
    "                    segment = clean_img[max(start-1, 0):idx, :]\n",
    "                    # if segment.shape[0] >= min_height:                    \n",
    "                    segments.append(segment)\n",
    "                elif axis == 'vertical':\n",
    "                    # Word segmentation\n",
    "                    segment = clean_img[:, max(start-1, 0):idx]\n",
    "                    # if segment.shape[1] >= min_width:\n",
    "                    segments.append(segment)\n",
    "                cnt = 0\n",
    "                start = -1\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_horizontal_projection(image, cut=3): \n",
    "    lines = projection_segmentation(image, axis='horizontal', cut=cut)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vertical_projection(line_image, cut=3):\n",
    "    line_words = projection_segmentation(line_image, axis='vertical', cut=cut)\n",
    "    line_words.reverse()\n",
    "    return line_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(img, visual=0):\n",
    "\n",
    "    lines = line_horizontal_projection(img)\n",
    "    words = []\n",
    "    \n",
    "    for idx, line in enumerate(lines):\n",
    "        \n",
    "        if visual:\n",
    "            # Check for the size of the line to be greater than 30\n",
    "            # if line.shape[0] > 30:\n",
    "            save_image(line, 'lines', f'line{idx}')\n",
    "\n",
    "        line_words = word_vertical_projection(line)\n",
    "        for w in line_words:\n",
    "            # if len(words) == 585:\n",
    "            #     print(idx)\n",
    "            words.append((w, line))\n",
    "        # words.extend(line_words)\n",
    "\n",
    "    # breakpoint()\n",
    "    if visual:\n",
    "        for idx, word in enumerate(words):\n",
    "            # check for the size of the word to be greater than 30\n",
    "            # print (word[0].shape)\n",
    "            # if word[0].shape[0] < 100 and word[0].shape[1] > 20 :\n",
    "            save_image(word[0], 'words', f'word{idx}')\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ› ï¸ Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hog(X_train_preprocess):\n",
    "    X_train_hog = []\n",
    "    for i in tqdm(X_train_preprocess):\n",
    "        X_train_hog.append(hog(i, orientations= 16, pixels_per_cell=(32, 32), cells_per_block=(4, 4), block_norm='L2-Hys'))\n",
    "    X_train_hog = np.array(X_train_hog)\n",
    "    return X_train_hog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sift(X_train_preprocess):\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    X_train_sift = []\n",
    "    for i in tqdm(X_train_preprocess):\n",
    "        kp, des = sift.detectAndCompute(i, None)\n",
    "        if des is None:\n",
    "            # Add a row of zeros to the SIFT descriptors\n",
    "            des = np.zeros((1, 128))\n",
    "        des = des.flatten()\n",
    "        X_train_sift.append(des)\n",
    "    return X_train_sift\n",
    "    \n",
    "# Pad the SIFT descriptors to the maximum length\n",
    "def pad_sift_descriptors(X_train_sift, fixed_len):\n",
    "    # Create a generator that yields each padded descriptor on-the-fly\n",
    "    padded_descriptors = (np.pad(des, (0, max(0, fixed_len - des.shape[0])))[:fixed_len] for des in X_train_sift)\n",
    "\n",
    "    # Convert the generator to a numpy array\n",
    "    X_train_sift_np = np.array(list(padded_descriptors))\n",
    "    return X_train_sift_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_sift = apply_sift(X_train_preprocess)\n",
    "\n",
    "# # Get the maximum keypoint length of the SIFT descriptors\n",
    "# max_kp = max(len(kp)/128 for kp in X_train_sift)\n",
    "\n",
    "# # Get the average keypoint length of the SIFT descriptors\n",
    "# avg_kp = np.mean([len(kp)/128 for kp in X_train_sift])\n",
    "\n",
    "# # Get the minimum keypoint length of the SIFT descriptors\n",
    "# min_kp = min(len(kp)/128 for kp in X_train_sift)\n",
    "\n",
    "# # Print the maximum keypoint length\n",
    "# print(max_kp)\n",
    "\n",
    "# # Print the average keypoint length\n",
    "# print(avg_kp)\n",
    "\n",
    "# # Print the minimum keypoint length\n",
    "# print(min_kp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  âœ˜ Unsuccessful Feature Extraction Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Laplacian_filter(img):\n",
    "    laplacian_filter = np.array([\n",
    "        [-1,-1,-1],\n",
    "        [-1,8,-1],\n",
    "        [-1,-1,-1]\n",
    "    ])\n",
    "    edge_image = convolve2d(img, laplacian_filter)\n",
    "    edge_image = np.where(edge_image > 0.5, edge_image, 0)\n",
    "    edge_image = np.where(edge_image < 0.5, edge_image, 255)\n",
    "\n",
    "    edge_image = 255 - edge_image\n",
    "    return edge_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edm1_matrix(edge_image):\n",
    "    edm_matrix = np.zeros((3,3))\n",
    "    edge_image = np.pad(edge_image, 1, mode='constant', constant_values=1)\n",
    "    for i in range(0, edge_image.shape[0]):\n",
    "        for j in range(0, edge_image.shape[1]):\n",
    "            if edge_image[i, j] == 0:\n",
    "                edm_matrix[1,1] += 1\n",
    "                if edge_image[i, j + 1] == 0:\n",
    "                    edm_matrix[1,2] += 1\n",
    "                if edge_image[i + 1, j + 1] == 0:\n",
    "                    edm_matrix[2,2] += 1\n",
    "                if edge_image[i + 1, j] == 0:\n",
    "                    edm_matrix[2,1] += 1\n",
    "                if edge_image[i + 1, j - 1] == 0:\n",
    "                    edm_matrix[2,0] += 1\n",
    "                if edge_image[i, j - 1] == 0:\n",
    "                    edm_matrix[1,0] += 1\n",
    "                if edge_image[i - 1, j - 1] == 0:\n",
    "                    edm_matrix[0,0] += 1\n",
    "                if edge_image[i - 1, j] == 0:\n",
    "                    edm_matrix[0,1] += 1\n",
    "                if edge_image[i - 1, j + 1] == 0:\n",
    "                    edm_matrix[0,2] += 1\n",
    "    return edm_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_values(edm1_matrix):\n",
    "    values = edm1_matrix.flatten()\n",
    "    edm1_occurrences_sorted = {}\n",
    "    edm1_occurrences_sorted[values[5]] = [5,3]\n",
    "    if values[2] not in edm1_occurrences_sorted:\n",
    "        edm1_occurrences_sorted[values[2]] = [2,6]\n",
    "    else:\n",
    "        edm1_occurrences_sorted[values[2]].extend([2,6])\n",
    "    if values[1] not in edm1_occurrences_sorted:\n",
    "        edm1_occurrences_sorted[values[1]] = [1,7]\n",
    "    else:\n",
    "        edm1_occurrences_sorted[values[1]].extend([1,7])\n",
    "    if values[0] not in edm1_occurrences_sorted:\n",
    "        edm1_occurrences_sorted[values[0]] = [0,8]\n",
    "    else:\n",
    "        edm1_occurrences_sorted[values[0]].extend([0,8])\n",
    "    edm1_occurrences_sorted = dict(sorted(edm1_occurrences_sorted.items(), reverse=True))\n",
    "\n",
    "    lst = []\n",
    "    for key in edm1_occurrences_sorted:\n",
    "        lst.extend(edm1_occurrences_sorted[key])\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_occurrence(neighboring_indices, edm1_occurrences_sorted_list):\n",
    "    for idx in edm1_occurrences_sorted_list:\n",
    "        if idx in neighboring_indices:\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edm2_matrix(edge_image, edm1_matrix):\n",
    "    edm2_matrix_flattened = np.zeros(9)\n",
    "    edm1_occurrences_sorted_list = sort_values(edm1_matrix)\n",
    "    edm2_matrix_flattened[4] = edm1_matrix[1,1]\n",
    "    edge_image = np.pad(edge_image, 1, mode='constant', constant_values=1)\n",
    "    for i in range(0, edge_image.shape[0]):\n",
    "        for j in range(0, edge_image.shape[1]):\n",
    "            neighboring_indices = []\n",
    "            if edge_image[i, j] == 0:\n",
    "                if edge_image[i, j + 1] == 0:\n",
    "                    neighboring_indices.append(5)\n",
    "                if edge_image[i - 1, j + 1] == 0:\n",
    "                    neighboring_indices.append(2)\n",
    "                if edge_image[i - 1, j] == 0:\n",
    "                    neighboring_indices.append(1)\n",
    "                if edge_image[i - 1, j - 1] == 0:\n",
    "                    neighboring_indices.append(0)\n",
    "                if edge_image[i, j - 1] == 0:\n",
    "                    neighboring_indices.append(3)\n",
    "                if edge_image[i + 1, j - 1] == 0:\n",
    "                    neighboring_indices.append(6)\n",
    "                if edge_image[i + 1, j] == 0:  \n",
    "                    neighboring_indices.append(7)\n",
    "                if edge_image[i + 1, j + 1] == 0:\n",
    "                    neighboring_indices.append(8)\n",
    "\n",
    "                first_occurrence = get_first_occurrence(neighboring_indices, edm1_occurrences_sorted_list)\n",
    "                edm2_matrix_flattened[first_occurrence] += 1\n",
    "    edm2_matrix = edm2_matrix_flattened.reshape(3,3)   \n",
    "    return edm2_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_edm(X_train_preprocess):\n",
    "    edge_images = [Laplacian_filter(img) for img in tqdm(X_train_preprocess)]\n",
    "    \n",
    "    edm1_matrices = [get_edm1_matrix(edge_img) for edge_img in tqdm(edge_images)]\n",
    "    edm2_matrices = [get_edm2_matrix(edge_images[i], edm1_matrices[i]) for i in tqdm(range(len(edge_images)))]\n",
    "    \n",
    "    edm1_matrices = np.array(edm1_matrices)\n",
    "    edm2_matrices = np.array(edm2_matrices)\n",
    "\n",
    "    edm1_matrices = edm1_matrices.reshape(-1,9)\n",
    "    edm2_matrices = edm2_matrices.reshape(-1,9)\n",
    "    return edm1_matrices, edm2_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_edm1, X_edm2 = apply_edm(X_train_preprocess)\n",
    "# print(X_edm1.shape)\n",
    "# print(X_edm2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDM_features = np.concatenate((X_edm1, X_edm2), axis=1)\n",
    "# print(EDM_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_additional_edm_features(X_edm1, X_edm2):\n",
    "    edges_direction = np.max(X_edm1, axis=1)\n",
    "    edges_direction = edges_direction.reshape(-1,1)\n",
    "    homogeneity = np.array([x/np.sum(x) for x in X_edm1])\n",
    "    pixel_regularity = np.array([x/x[4] for x in X_edm1])\n",
    "    edges_regularity = np.array([x / x[4] for x in X_edm2])\n",
    "    return edges_direction, homogeneity, pixel_regularity, edges_regularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edges_direction, homogeneity, pixel_regularity, edges_regularity = apply_additional_edm_features(X_edm1, X_edm2)\n",
    "# print(edges_direction.shape)\n",
    "# print(homogeneity.shape)\n",
    "# print(pixel_regularity.shape)\n",
    "# print(edges_regularity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edm_features = np.concatenate((edges_direction, homogeneity, pixel_regularity, edges_regularity), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš™ï¸ Preprocessing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    def __init__(self, preprocess_pipe):\n",
    "        self.preprocess_pipe = preprocess_pipe\n",
    "        \n",
    "    def preprocess_data(self, X, test=False):\n",
    "        fixed_len = 128 * 350\n",
    "        X_preprocess = [preprocess(i) for i in tqdm(X)]\n",
    "        X_preprocess = np.array(X_preprocess)\n",
    "        X_hog = apply_hog(X_preprocess)\n",
    "        X_sift = apply_sift(X_preprocess)\n",
    "        X_sift_padded = pad_sift_descriptors(X_sift, fixed_len)\n",
    "        X_features = np.concatenate((X_hog, X_sift_padded), axis=1)\n",
    "        if test:\n",
    "            X_features_transformed = self.preprocess_pipe.transform(X_features)\n",
    "        else:\n",
    "            X_features_transformed = self.preprocess_pipe.fit_transform(X_features)\n",
    "            with open('preprocess_pipe.pkl', 'wb') as f:\n",
    "                pickle.dump(self.preprocess_pipe, f)\n",
    "        return X_features_transformed\n",
    "    \n",
    "    def preprocess_test_data(self, X):\n",
    "        fixed_len = 128 * 350\n",
    "        X_preprocess = preprocess(X)\n",
    "        X_preprocess = [np.array(X_preprocess)]\n",
    "        X_hog = apply_hog(X_preprocess)\n",
    "        X_sift = apply_sift(X_preprocess)\n",
    "        X_sift_padded = pad_sift_descriptors(X_sift, fixed_len)\n",
    "        X_features = np.concatenate((X_hog, X_sift_padded), axis=1)\n",
    "        X_features_transformed = self.preprocess_pipe.transform(X_features)\n",
    "        return X_features_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "<a id='4.11'></a>\n",
    "<p style=\"font-size: 34px; color: #FFFFFF; /* Set text color to black */ \n",
    "  font-family: 'Roboto'; \n",
    "  text-align: center; \n",
    "  padding: 10px 20px; /* Add padding for spacing */\n",
    "  background-image: linear-gradient(to right, #9746ff, #000000); \n",
    "  border-radius: 5px 5px;\"><strong>PyTorch Model</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, learning_rate=0.0002 , epoch=50):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.output_dim = output_dim\n",
    "        self.best_accuracy = -1  # Initialize with a value that will definitely be improved upon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epoch = epoch\n",
    "        self.model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim1, self.hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim2, self.output_dim),\n",
    "        )\n",
    "        return model\n",
    "        \n",
    "    \n",
    "    def fit(self, X_train_features, X_val_features, y_train_labels, y_val_labels, labels):\n",
    "        y_train =  [labels.index(i) for i in y_train_labels]\n",
    "        y_val = [labels.index(i) for i in y_val_labels]\n",
    "        \n",
    "        lb = LabelBinarizer()\n",
    "        y_train_one_hot = lb.fit_transform(y_train)\n",
    "        y_val_one_hot = lb.fit_transform(y_val)\n",
    "        \n",
    "        X_train_tensor = torch.FloatTensor(X_train_features)\n",
    "        y_train_tensor = torch.LongTensor(y_train_one_hot)\n",
    "\n",
    "        X_val_tensor = torch.FloatTensor(X_val_features)\n",
    "        y_val_tensor = torch.LongTensor(y_val_one_hot)\n",
    "        \n",
    "        # Create a dataset\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "        # Create a dataloader\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Create a tqdm object\n",
    "        progress_bar = tqdm(range(self.epoch), desc=\"Epoch\", leave=False)\n",
    "\n",
    "        for self.epoch in progress_bar:\n",
    "            total_loss = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(X_batch)\n",
    "                loss = criterion(outputs, torch.max(y_batch, 1)[1])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy on the validation set\n",
    "            val_acc = 0\n",
    "            with torch.no_grad():\n",
    "                for X_val, y_val in test_loader:\n",
    "                    outputs = self.model(X_val)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_acc += (predicted == torch.max(y_val, 1)[1]).sum().item()\n",
    "            accuracy = val_acc / len(y_val_tensor)\n",
    "\n",
    "            # If the current model has better accuracy, save the model parameters\n",
    "            if accuracy > self.best_accuracy:\n",
    "                self.best_accuracy = accuracy\n",
    "                self.best_model_state = deepcopy(self.model.state_dict(prefix=\"model.\"))\n",
    "\n",
    "            # Update the progress bar\n",
    "            progress_bar.set_postfix({'Loss': f'{total_loss:.4f}', 'Accuracy': f'{self.best_accuracy:.4f}'})\n",
    "            \n",
    "        # Save the best model parameters to the model\n",
    "        self.save_best_model('best_model.pth')\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(X_tensor)\n",
    "        _, predicted = torch.max(predictions, 1)\n",
    "        return predicted.numpy()\n",
    "\n",
    "    def save_best_model(self, filepath):\n",
    "        torch.save(self.best_model_state, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’ª Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Scheherazade New', 'Marhey', 'Lemonada', 'IBM Plex Sans Arabic']\n",
    "\n",
    "preprocess_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.99)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_labels, _ = load_images()\n",
    "\n",
    "with open('X_data.pkl', 'wb') as f:\n",
    "    pickle.dump(X_data, f)\n",
    "\n",
    "with open('y_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(y_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('X_data.pkl', 'rb') as f:\n",
    "#     X_data = pickle.load(f)\n",
    "\n",
    "# with open('y_labels.pkl', 'rb') as f:\n",
    "#     y_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_labels, test_size=0.20, random_state=42, stratify=y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3197 3197\n",
      "800 800\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(y_train))\n",
    "print(len(X_val), len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "        \n",
    "with open('X_val.pkl', 'wb') as f:\n",
    "    pickle.dump(X_val, f)\n",
    "        \n",
    "with open('y_train.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "        \n",
    "with open('y_val.pkl', 'wb') as f:\n",
    "    pickle.dump(y_val, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('X_train.pkl', 'rb') as f:\n",
    "#     X_train = pickle.load(f)\n",
    "    \n",
    "# with open('X_val.pkl', 'rb') as f:\n",
    "#     X_val = pickle.load(f)\n",
    "    \n",
    "# with open('y_train.pkl', 'rb') as f:\n",
    "#     y_train = pickle.load(f)\n",
    "    \n",
    "# with open('y_val.pkl', 'rb') as f:\n",
    "#     y_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_module = Preprocessing(preprocess_pipe)\n",
    "X_train_features = preprocess_module.preprocess_data(X_train)\n",
    "with open('X_train_features.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train_features, f)\n",
    "    \n",
    "input_dim = X_train_features.shape[1]\n",
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_features = preprocess_module.preprocess_data(X_val, test=True)\n",
    "with open('X_val_features.pkl', 'wb') as f:\n",
    "    pickle.dump(X_val_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('preprocess_pipe.pkl', 'rb') as f:\n",
    "#     preprocess_pipe = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('X_train_features.pkl', 'rb') as f:\n",
    "#     X_train_features = pickle.load(f)\n",
    "    \n",
    "# with open('X_val_features.pkl', 'rb') as f:\n",
    "#     X_val_features = pickle.load(f)\n",
    "    \n",
    "# with open('y_train.pkl', 'rb') as f:\n",
    "#     y_train = pickle.load(f)\n",
    "        \n",
    "# with open('y_val.pkl', 'rb') as f:\n",
    "#     y_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = PyTorchClassifier(input_dim, 512, 256, len(labels), learning_rate=0.00025, epoch=50)\n",
    "pytorch_model.fit(X_train_features, X_val_features, y_train, y_val, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš¨ Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_pred, y_test):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "for i in tqdm(labels):\n",
    "    for filename in os.listdir(f'content/train/{i}'):\n",
    "        img = cv2.imread(f'content/train/{i}/{filename}', cv2.IMREAD_GRAYSCALE)\n",
    "        X_test.append(img)\n",
    "        y_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  [labels.index(i) for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyTorchClassifier(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=2981, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('preprocess_pipe.pkl', 'rb') as f:\n",
    "    preprocess_pipe = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_module = Preprocessing(preprocess_pipe)\n",
    "X_test_features_transformed = preprocess_module.preprocess_data(X_test, test=True)\n",
    "\n",
    "pytorch_classifier = PyTorchClassifier(input_dim, 512, 256, len(labels), learning_rate=0.00025, epoch=50)\n",
    "pytorch_classifier.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "pytorch_classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in pytorch_classifier.state_dict():\n",
    "    print(param_tensor, \"\\t\", pytorch_classifier.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pytorch_classifier.predict(X_test_features_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.5000%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ðŸ“Š Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_accuracy = {\n",
    "    \"PyTorch Neural Network\": 96.5,\n",
    "    \"Stacking Classifier\": 96,\n",
    "    \"Logistic Regression\": 95.5,\n",
    "    \"MLP Classifier\": 92.75,\n",
    "    \"SVM\": 91.25,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Score=%{marker.color}<br>Model=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": [
           96.5,
           96,
           95.5,
           92.75,
           91.25
          ],
          "coloraxis": "coloraxis",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "h",
         "showlegend": false,
         "textposition": "auto",
         "type": "bar",
         "x": [
          96.5,
          96,
          95.5,
          92.75,
          91.25
         ],
         "xaxis": "x",
         "y": [
          "PyTorch Neural Network",
          "Stacking Classifier",
          "Logistic Regression",
          "MLP Classifier",
          "SVM"
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "Score"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#f2f5fa"
            },
            "error_y": {
             "color": "#f2f5fa"
            },
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "baxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#506784"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "header": {
             "fill": {
              "color": "#2a3f5f"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#f2f5fa",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#f2f5fa"
          },
          "geo": {
           "bgcolor": "rgb(17,17,17)",
           "lakecolor": "rgb(17,17,17)",
           "landcolor": "rgb(17,17,17)",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#506784"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "dark"
          },
          "paper_bgcolor": "rgb(17,17,17)",
          "plot_bgcolor": "rgb(17,17,17)",
          "polar": {
           "angularaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "radialaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "yaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "zaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#f2f5fa"
           }
          },
          "sliderdefaults": {
           "bgcolor": "#C8D4E3",
           "bordercolor": "rgb(17,17,17)",
           "borderwidth": 1,
           "tickwidth": 0
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "caxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "updatemenudefaults": {
           "bgcolor": "#506784",
           "borderwidth": 0
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Models Comparison"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Score"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Model"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = pd.DataFrame(models_accuracy.items(), columns=['Model', 'Score'])\n",
    "models = models.sort_values(by = 'Score', ascending = False)\n",
    "\n",
    "px.bar(data_frame = models, x = 'Score', y = 'Model', color = 'Score', template = 'plotly_dark', title = 'Models Comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build a shallow neural network model of 1 hidden layer with 256 neurons and relu activation function\n",
    "# # Create a model of 1 hidden layer with 256 neurons and relu activation function and adam solver and softmax output layer\n",
    "# model = MLPClassifier(hidden_layer_sizes=(256,), activation='relu', solver='adam', verbose=True)\n",
    "\n",
    "# # Fit the model\n",
    "# model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# # Predict the training data\n",
    "# y_train_pred = model.predict(X_train_transformed)\n",
    "\n",
    "# # Predict the testing data\n",
    "# y_test_pred = model.predict(X_test_transformed)\n",
    "\n",
    "# # Print the accuracy of the model\n",
    "# print('Train accuracy: ', accuracy_score(y_train, y_train_pred)*100)\n",
    "\n",
    "# print('Test accuracy: ', accuracy_score(y_test, y_test_pred)*100)\n",
    "\n",
    "# # Print the classification report\n",
    "# print('Train classification report: ', classification_report(y_train, y_train_pred, target_names=labels))\n",
    "\n",
    "# print('Test classification report: ', classification_report(y_test, y_test_pred, target_names=labels))\n",
    "\n",
    "# # Accuracy: 92.75%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lin_model = LogisticRegression()\n",
    "\n",
    "# # Define the hyperparameters\n",
    "# param_grid = {\n",
    "#     'C': np.logspace(-4, 4, 30),\n",
    "#     'penalty': ['l2'],\n",
    "#     'solver': ['liblinear', 'saga', 'lbfgs'],\n",
    "#     'warm_start': [True, False]\n",
    "# }\n",
    "\n",
    "# # Initialize the RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(lin_model, param_distributions=param_grid, n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# # Fit the model\n",
    "# random_search.fit(X_train_transformed, y_train)\n",
    "# relevant_columns = ['param_C', 'param_penalty', 'param_solver', 'param_warm_start', 'mean_test_score', 'std_test_score', 'rank_test_score']\n",
    "# cv_results_df = pd.DataFrame(random_search.cv_results_)[relevant_columns].round(decimals=3).sort_values(by='rank_test_score')\n",
    "# cv_results_df.head(10)\n",
    "\n",
    "# # Print the best parameters\n",
    "# print(random_search.best_params_)\n",
    "\n",
    "# Best parameters of Logistic Regression: \n",
    "# {'warm_start': True, 'solver': 'saga', 'penalty': 'l2', 'C': 0.001}\n",
    "# {'warm_start': True, 'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.0006723357536499335}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the Logistic Regression model\n",
    "# model = LogisticRegression(warm_start=True, solver='saga', penalty='l2', C=0.8, random_state=42)\n",
    "\n",
    "# # Fit the model\n",
    "# model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# # Predict the test data\n",
    "# y_pred = model.predict(X_test_transformed)\n",
    "\n",
    "# # Predict the train data\n",
    "# y_pred_train = model.predict(X_train_transformed)\n",
    "\n",
    "# # Print the training accuracy\n",
    "# print(f\"Training Accuracy: {accuracy_score(y_train, y_pred_train)*100}\")\n",
    "\n",
    "# # Print the testing accuracy\n",
    "# print(f\"Testing Accuracy: {accuracy_score(y_test, y_pred)*100}\")\n",
    "\n",
    "# # Print the f1 score of the training data\n",
    "# print(f\"Training F1 Score: {f1_score(y_train, y_pred_train, average='weighted')}\")\n",
    "\n",
    "# # Print the f1 score of the testing data\n",
    "# print (f\"Testing F1 Score: {f1_score(y_test, y_pred, average='weighted')}\")\n",
    "\n",
    "# # Print the classification report of the training data\n",
    "# print(classification_report(y_train, y_pred_train, target_names=labels))\n",
    "\n",
    "# # Print the classification report of the testing data\n",
    "# print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "# # Save the model\n",
    "# joblib.dump(model, 'logistic_model.pkl')\n",
    "\n",
    "# # Accuracy: 95.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm = SVC()\n",
    "\n",
    "# param_dist = {\n",
    "#     'C': np.logspace(-3, 3, 15), \n",
    "#     'kernel': ['poly', 'rbf'], \n",
    "#     'degree': [2, 3],\n",
    "#     'gamma': ['scale', 'auto']\n",
    "# }\n",
    "\n",
    "# clf_searched = RandomizedSearchCV(svm, param_dist, n_iter=100, cv=5, random_state=42, n_jobs=-1, verbose=1)\n",
    "\n",
    "# clf_searched.fit(X_train_transformed, y_train)\n",
    "# relevant_columns = ['param_C', 'param_kernel', 'param_gamma', 'mean_test_score', 'std_test_score', 'rank_test_score']\n",
    "# cv_results_df = pd.DataFrame(clf_searched.cv_results_)[relevant_columns].round(decimals=3).sort_values(by='rank_test_score')\n",
    "# cv_results_df.head(10)\n",
    "\n",
    "# Best parameters of the SVM\n",
    "# {C : 7.196857 , kernel : 'rbf', gamma : 'scale'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the SVM model\n",
    "# svm = SVC(C=7.196857 , kernel='rbf', gamma='scale', random_state=42)\n",
    "\n",
    "# # Fit the SVM on the training data\n",
    "# svm.fit(X_train_transformed, y_train)\n",
    "\n",
    "# # Predict the labels of the test set\n",
    "# y_pred = svm.predict(X_test_transformed)\n",
    "# # Predict the labels of the training set\n",
    "# y_pred_train = svm.predict(X_train_transformed)\n",
    "\n",
    "# # Print the accuracy of the SVM model on the training set\n",
    "# print(f\"Accuracy of SVM model on the training set: {accuracy_score(y_train, y_pred_train)*100}\")\n",
    "\n",
    "# # Print the accuracy of the SVM model on the test set\n",
    "# print(f\"Accuracy of SVM model: {accuracy_score(y_test, y_pred)*100}\")\n",
    "\n",
    "# # Print the classification report of the SVM model\n",
    "# print(classification_report(y_train, y_pred_train, target_names=labels))\n",
    "\n",
    "# # Print the classification report of the SVM model\n",
    "# print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "# # Save the SVM model as h5 file\n",
    "# joblib.dump(svm, 'svm_model.pkl')\n",
    "\n",
    "# # Accuracy: 91.25%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
